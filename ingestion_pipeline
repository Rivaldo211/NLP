#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import logging
#initialise le yaml file
import yaml
from os.path import split

from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from datetime import datetime

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_groq import ChatGroq

# Logging Configuration
formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(funcName)-30s %(message)s')
fh = logging.FileHandler("main.log")
sh = logging.StreamHandler()
fh.setLevel(logging.DEBUG)
sh.setLevel(logging.INFO)
fh.setFormatter(formatter)
sh.setFormatter(formatter)

# load Api key
load_dotenv()

# Paths
INDEX_PATH = "./storage/faiss_db"
DATABASE_PATH = "C:/Users/Rivaldo/Documents/ATIS/orkl.eu-2025-09-25-txtonly-subset/"

def embed_documents(doc_path: str):
    if os.path.exists(doc_path):
        loader = DirectoryLoader(doc_path, glob="*.txt", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"})
        docs = loader.load()
        logging.info("DATABASE Loaded ")

        # split
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)
        return splits
    else:
        return logging.error("Path Error")

def init_retriever():
    try:
        # Prepare Documents
        splits = embed_documents(DATABASE_PATH)

        # Generate embeddings
        embeddings = OpenAIEmbeddings()

        # load or create FAISS-Base
        if os.path.exists(INDEX_PATH):
            logging.info("Loading existing FAISS database...")
            vectordb = FAISS.load_local(
                folder_path=INDEX_PATH,
                embeddings=embeddings,
                allow_dangerous_deserialization=True
            )
        else:
            logging.info("Creating a new FAISS database...")
            vectordb = FAISS.from_documents(splits, embeddings)
            vectordb.save_local(folder_path=INDEX_PATH)

        # Return retriever
        return vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 4, "fetch_k": 8}
        )

    except Exception as e:
        logging.error(f"Error initializing the retriever : {str(e)}")
        raise

# save Metadaten
def _info(embedding_model: str, chunk_size=1000, chunk_overlap=200):
    metadaten_docs= []
    splits = embed_documents(DATABASE_PATH)
    now = datetime.now().isoformat()

    for doc in splits:
        new_metadata = doc.metadata.copy()
        new_metadata.update({
            "embedding_model": embedding_model,
            "created_at": now,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
        })
        metadaten_docs.append(type(doc)(
            page_content=doc.page_content,
            metadata=new_metadata
        ))
    return metadaten_docs


SYSTEM_PROMPT_TEMPLATE = """
        You are a factual assistant. Use ONLY the provided CONTEXT to answer the question.
        context:  {context}
        question: {question} 
        If the information is not present in the CONTEXT, respond exactly with 'Das weiss ich nicht'.
        Keep answers concise and, when appropriate, cite the source(s) in brackets.
        Always say 'Hello' at the beginning of the answer.
        Always say 'thanks for asking' at the end of the answer
"""

API_KEY = os.getenv("API_KEY")

class LLM:

    def __init__(self):
        self.prompt = ChatPromptTemplate.from_template(SYSTEM_PROMPT_TEMPLATE)
        logging.info("Initialisation du retriever...")
        self.retriever = init_retriever()
        logging.info("Vector base is ready.")

    def get_llm(self):
        return ChatGroq(model="x-ai/grok-4.1-fast:free", api_key=API_KEY, temperature=0.8)

    def answer(self, query: str) -> str:

        def format_docs(docs):
            return "\n".join(doc.page_content for doc in docs)

        chain =   {"question": RunnablePassthrough(), "context": self.retriever } | self.prompt | self.get_llm() | StrOutputParser()
        return chain.invoke(query)

def main():
    logging.info("Welcome to a CTI Assistant")
    try:
        assistant = LLM()
        logging.info(f"Enter 'quit' or 'exit' to exit.")
        while True:
            query = input("\nQuestion: ")
            if query.lower() in ["quit", "exit"]:
                logging.info(f"Exiting CTI Assistant...")
                break

            try:
                answer = assistant.answer(query)
                logging.info(f"\nAnswer: {answer}")
            except Exception as e:
                logging.error(f"Error to generate answer : {str(e)}")

    except Exception as e:
        logging.error(f"Error to initialise the Llm : {str(e)}")

if __name__ == "__main__":
    main()
