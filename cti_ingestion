import os
import logging
from os.path import split

from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from datetime import datetime

# Configure logging
formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(funcName)-30s %(message)s')
fh = logging.FileHandler("main.log")
sh = logging.StreamHandler()
fh.setLevel(logging.DEBUG)
sh.setLevel(logging.INFO)
fh.setFormatter(formatter)
sh.setFormatter(formatter)

logger = logging.getLogger(__name__)
logger.addHandler(fh)
logger.addHandler(sh)
logger.setLevel(logging.DEBUG)
logger.propagate = False

# Charger les variables d'environnement
load_dotenv()

# Paths
INDEX_PATH = "./storage/faiss_db"
DATABASE_PATH = "C:/Users/Rivaldo/Documents/ATIS/orkl.eu-2025-09-25-txtonly-subset/"

def embed_documents(doc_path: str):
    # load all .txt in DATABASE_PATH
    loader = DirectoryLoader(DATABASE_PATH, glob="*.txt", loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"})
    docs = loader.load()

    # split
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(docs)
    return splits

def init_retriever():
    try:
        # Prepare Documents
        splits = embed_documents(DATABASE_PATH)

        # Generate embeddings
        embeddings = OpenAIEmbeddings()

        # load or create FAISS-Base
        if os.path.exists(INDEX_PATH):
            logger.info("Loading existing FAISS database...")
            vectordb = FAISS.load_local(
                folder_path=INDEX_PATH,
                embeddings=embeddings,
                allow_dangerous_deserialization=True
            )
        else:
            logger.info("Creating a new FAISS database...")
            vectordb = FAISS.from_documents(splits, embeddings)
            vectordb.save_local(folder_path=INDEX_PATH)

        # Return retriever
        return vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 4, "fetch_k": 8}
        )

    except Exception as e:
        logger.error(f"Error initializing the retriever : {str(e)}")
        raise
    
# save Metadaten
def _info(embedding_model: str, chunk_size=1000, chunk_overlap=200):
    metadaten_docs= []
    splits = embed_documents(DATABASE_PATH)
    now = datetime.now().isoformat()

    for doc in splits:
        new_metadata = doc.metadata.copy()
        new_metadata.update({
            "embedding_model": embedding_model,
            "created_at": now,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
        })
        metadaten_docs.append(type(doc)(
            page_content=doc.page_content,
            metadata=new_metadata
        ))
    return metadaten_docs

if __name__ == "__main__":
    retriever = init_retriever()
    print(" Vector base is ready.")
